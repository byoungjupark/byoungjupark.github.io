---
title: "웹 스크래핑"
excerpt: "파이썬 Requests, Selenium, BeautifulSoup 모듈을 이용한 웹 스크래핑"

---

웹 스크래핑은 웹페이지에서 원하는 데이터를 추출하는 것을 말한다. 
파이썬의 여러 모듈들을 이용해서 웹 스크래핑을 할 수 있다.

웹 스크래핑을 하려면 먼저 웹페이지를 불러와야 한다. 다르게 말하면 웹페이지의 html 코드를 가져오는 것이기도 하다.
그 다음 html에서 특정 데이터를 추출한다. 
# 1. 웹페이지 가져오기 (Requests와 Selenium)
html 코드를 가져오는 모듈 2가지가 있다.
Requests는 html을 읽어오고 빠르지만 동적인 웹페이지에서 사용하기엔 힘들다.
Selenium은 동적인 페이지에서 유용하다. 클릭, 검색을 하거나 페이지 화면이 바뀌어야 할 때 쓸 수 있다. 다만 느리다.

requests로 구글 홈페이지를 불러와보고, selenium으로 네이버 로그인을 해보자.
## 1-1 Reauests로 구글 홈페이지 가져오기
```python
import requests
res = requests.get("http://google.com")
res.raise_for_status()

with open("mygoogle.html", "w", encoding="utf8") as f:
    f.write(res.text)
```
* requests 모듈을 불러온다.
* get은 "http://google.com" 이라는 url에 접속해서 html코드를 가져오겠다는 메서드이다. 해당 서버는 나의 요청(클라이언트의 요청)에 응답할 것이고, 이상이 없다면 html정보를 제공할 것이다. 그 다음 res라는 객체로 만들었다.
* raise_for_status는 해당 서버의 응답이 무엇인지를 알려주는 메서드이다. 이 메서드를 출력했을 때, 200이 나오면 정상적으로 서버가 응답했다는 의미이다.
* res객체를 text 형태로 전환시켜 mygoogle.html 파일을 만들어 그 안에 넣어 주었다. 

## 1-2 Seleium으로 네이버 로그인하기
```python
import selenium from webdriver
from selenium.webdriver.common import keys
from webdriver_manager.chrome import ChromeDriverManager

browser = webdriver.Chrome(ChromeDriverManager().install())

# 1. 네이버 이동
browser.get("http://naver.com")

# 2. 로그인 버튼 클릭
elem = browser.find_element_by_class_name("link_login")
elem.click()

# 3. id, pw 입력
browser.find_element_by_id("id").send_keys("naver_id")
browser.find_element_by_id("pw").send_keys("password")

# 4. 로그인 버튼 클릭
browser.find_element_by_id("log.login").click()

# 5. id를 새로 입력
browser.find_element_by_id("id").clear()
browser.find_element_by_id("id").send_keys("my_id")

# 6. 브라우저 종료
browser.quit()
```
selenium을 사용하려면 2가지를 설치해야 한다.
1. selenium 설치 : 파이썬 터미널에서 설치할 수 있다. pip install selenium 
2. webdriver 다운로드 : 자신이 주 웹브라우저 버전에 맞는 웹드라이버를 다운로드한다. 저장위치는 같은 경로로 해주면 편하다.
___
나의 경우 같은 디렉토리 안에 넣었는데도, 'chromedriver' executable needs to be in PATH 에러가 발생하였다. **webdriver-manager** 라이브러리를 설치하면 자동으로 driver의 경로를 설정해준다.
___
* selenium, selenium의 keys모듈, webdriver_manager를 불러온다. 
* 나의 주 웹브라우저는 크롬이므로 조금 전 설치했던 chromedriver.exe를 불러온다. chromedriver.exe가 같은 디렉토리에 있다면 입력 생략해도 된다. 나의 경우 webdriver-manager를 이용할 것이기 때문에 괄호 안에 ChromeDriverManager().install()를 입력한다. 그리고 불러온 크롬 드라이버를 browser 변수에 저장했다.
* get 메서드로 네이버 웹사이트를 불러온다.
* 개발자 도구를 사용하여 내가 원하는 로그인 버튼, 아이디, 비밀번호 등의 html element를 확인할 수 있다. 로그인 버튼은 class 이름으로 필터링하여 찾을 수 있다. 그 다음 elem 변수로 저장 후 click 메서드로 로그인 버튼을 클릭한다.
* 아이디와 비밀번호는 send_keys 메서드에 입력한다.
* 아이디를 새로 입력하려면 clear 메서드로 기존 아이디를 지운 다음, send_keys 메서드로 다시 입력한다.
* 네이버 웹페이지를 닫으려면 quit 메서드를 사용한다. (close 메서드는 현재 탭만 닫아준다.)


# 2. 웹페이지의 데이터 추출하기 (BeautifulSoup)
```python
import requests 
import bs4 from BeautifulSoup

url = "https://comic.naver.com/webtoon/weekday.nhn"
res = requests.get(url)
res.raise_for_status()
soup = BeautifulSoup(res.text, "lxml")

#네이버 웹툰 전체목록 가져오기

#class 속성이 title인 모든 a element 를 반환
cartoons = soup.find_all("a", attrs={"class":"title"})
# a element안의 값들을 출력
for cartoon in cartoons:
    print(cartoon.get_text())
```
웹페이지를 가져온 후 BeautifulSoup 모듈을 이용해 그 안에서 원하는 데이터를 추출할 수 있다. 사전에 BeautifulSoup 모듈과 lxml 모듈을 설치해야 한다.

_XML(eXtensible Markup Language)은 확장할 수 있는 마크업 언어이며, html은 xml의 일종이다. xml을 해석하는 프로그램을 parser라고 하고 여러 종류가 있다. 파이썬에서 주로 사용하는 xml parser는 lxml이다._

BeautifulSoup을 이용해 네이버 웹툰 페이지의 웹툰 제목들을 추출해보자.

* requests와 함께 BeautifulSoup 모듈도 불러온다.
* 네이버 웹툰 페이지 주소를 url 변수로 저장한다.
* 우선 requests의 get 메서드로 위 url을 불러오고 서버가 요청에 정상적으로 응답했는지 확인한다.
* BeautifulSoup으로 네이버 웹툰 페이지를 텍스트 형태로 전환시키고, 해석은 lxml이 한다. 그리고 이를 soup 객체로 만들어 준다.
* 개발자 도구를 이용해 웹툰 각각의 제목들이 어떤 element로 묶여있는지 확인한다.
* find_all 메서드로 a element 중에서 class가 title인 것들을 cartoons 변수로 담는다. find_all로 찾은 값들은 리스트 형태이다.
* for문으로 조건에 맞는 a element들 안의 텍스트값을 추출한다.
